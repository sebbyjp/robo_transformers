{
    "summary": "The code includes two classes, performs inference on a Vision Language Action model, handles exceptions, and predicts grasps from point cloud data using a model. It applies sigmoid functions, selects successful grasps based on thresholds, and displays results if needed.",
    "details": [
        {
            "comment": "This code defines a class InvertingDummyAction which returns an inverted action on each call, and the class InferenceServer for an inference server with model type and weight key options.",
            "location": "\"/media/root/Prima/works/robo_transformers/docs/src/robo_transformers/inference_server.py\":0-39",
            "content": "from PIL import Image\nimport numpy as np\nfrom typing import Optional, TypeVar\nfrom pprint import pprint\nfrom absl import logging\nfrom beartype import beartype\nfrom dataclasses import asdict, fields\nfrom numpy.typing import ArrayLike\nfrom robo_transformers.registry import REGISTRY\nfrom robo_transformers.abstract.action import  Action\nfrom robo_transformers.abstract.agent import Agent\nActionT = TypeVar('ActionT', bound=Action)\n@beartype\nclass InvertingDummyAction:\n    '''Returns Dummy Action that inverts its action every call.\n    '''\n    def __init__(self, action: ActionT):\n        self.action: ActionT = action\n    def invert_(self):\n        for f in fields(self.action):\n            setattr(self.action, f.name, -getattr(self.action, f.name))\n    def __post_init__(self):\n        # First call will invert again.\n        self.invert_()\n    def make(self) -> dict:\n        self.invert_()\n        return asdict(self.action)\n@beartype\nclass InferenceServer:\n    def __init__(self,\n                 model_type: str = \"rt1\",\n                 weights_key: str = \"rt1main\","
        },
        {
            "comment": "Initializes the inference server with optional arguments for model type, weights key, dummy action, and custom agent. If a dummy action is specified, it returns an action that inverts every call. Otherwise, if a custom agent is provided, it uses that; otherwise, it initializes the agent based on the model type and weights key. The __call__ method is also defined but not explained in this chunk of code.",
            "location": "\"/media/root/Prima/works/robo_transformers/docs/src/robo_transformers/inference_server.py\":40-68",
            "content": "                 dummy: bool = False,\n                 agent: Optional[Agent] = None,\n                 **kwargs\n                 ):\n        '''Initializes the inference server.\n        Args:\n            model_type (str, optional): Defaults to \"rt1\".\n            weights_key (str, optional): Defaults to \"rt1main\".\n            dummy (bool, optional): If true, a dummy action will be returned that inverts every call. Defaults to False.\n            agent (VLA, optional): Custom agent that implements VLA interface. Defaults to None.\n            **kwargs: kwargs for custom agent initialization.\n        '''\n        self.dummy: bool = dummy\n        if dummy:\n            self.action = InvertingDummyAction(REGISTRY[model_type]['action']())\n            return\n        elif agent is not None:\n            self.agent: Agent = agent\n        else:\n            self.agent: Agent = REGISTRY[model_type]['agent'](weights_key)\n            self.action = REGISTRY[model_type]['action']()\n    def __call__(self,\n                 save: bool = False,"
        },
        {
            "comment": "This code snippet is responsible for running inference on a Vision Language Action model. It accepts optional arguments such as image, save, *args, and **kwargs. If an image is provided and the save flag is set to True, it saves the observed image. The agent's action is then retrieved by calling the act() method on the agent object with the provided kwargs. If logging verbosity is high and an instruction is present in the kwargs, it prints the instruction and the resulting action. If any exception occurs during this process, it prints the traceback.",
            "location": "\"/media/root/Prima/works/robo_transformers/docs/src/robo_transformers/inference_server.py\":69-97",
            "content": "                 **kwargs\n                 ) -> dict:\n        '''Runs inference on a Vision Language Action model.\n        Args:\n            save (bool, optional): Whether or not to save the observed image. Defaults to False.\n            *args: args for the agent.\n            **kwargs: kwargs for the agent.\n        Returns:\n            dict: See RT1Action for details.\n        '''\n        image: ArrayLike = kwargs.get('image')\n        if image is not None and save:\n            Image.fromarray(np.array(image, dtype=np.uint8)).save(\"rt1_saved_image.png\")\n        if not self.dummy:\n            try:\n                self.action = self.agent.act(**kwargs)\n                if logging.get_verbosity() > logging.DEBUG and kwargs.get('instruction') is not None:\n                    intruction = kwargs['instruction']\n                    print(f'instruction: {intruction}')\n                    pprint(self.action)\n            except Exception as e:\n                import traceback\n                traceback.print_tb(e.__traceback__)"
        },
        {
            "comment": "This code snippet is part of the RoboTransformers project and contains a class `Rt1Observer` that observes data from various sources. The `inference` function takes multiple arguments such as model, internal state, observation, supervision, and configuration, and returns an output in form of a dictionary. The code then initializes a ContactGraspNet (CGN) model, processes point cloud data, and performs inference to generate grasps, confidence scores, and point indices. If the number of points in the processed data exceeds 20000, it raises an exception.",
            "location": "\"/media/root/Prima/works/robo_transformers/docs/src/robo_transformers/inference_server.py\":98-132",
            "content": "                raise e\n        return self.action.make()\n# class Rt1Observer(Observer):\n#     def observe(self, srcs: list[Src(PIL.Image), Src(str)]) -> Observation:\n#         pass\n# def inference(\n#     model: any,\n#     internal_state: dict,\n#     observation: dict,\n#     supervision: dict,\n#     config: dict,\n# ) -> dict:\n#     \"\"\"Infer action from observation.\n#     Args:\n#         cgn (CGN): ContactGraspNet model\n#         pcd (np.ndarray): point cloud\n#         threshold (float, optional): Success threshol. Defaults to 0.5.\n#         visualize (bool, optional): Whether or not to visualize output. Defaults to False.\n#         max_grasps (int, optional): Maximum grasps. Zero means unlimited. Defaults to 0.\n#         obj_mask (np.ndarray, optional): Object mask. Defaults to None.\n#     Returns:\n#         tuple[np.ndarray, np.ndarray, np.ndarray]: The grasps, confidence and indices of the points used for inference.\n#     \"\"\"\n# cgn.eval()\n# pcd = torch.Tensor(pcd).to(dtype=torch.float32).to(cgn.device)\n# if pcd.shape[0] > 20000:"
        },
        {
            "comment": "Code samples points, grasps, and widths predictions from a model given point cloud data and other parameters. It applies a sigmoid function to the confidence scores and reshapes them before extracting relevant results into numpy arrays for further use.",
            "location": "\"/media/root/Prima/works/robo_transformers/docs/src/robo_transformers/inference_server.py\":133-166",
            "content": "#     downsample_idxs = np.array(random.sample(range(pcd.shape[0] - 1), 20000))\n# else:\n#     downsample_idxs = np.arange(pcd.shape[0])\n# pcd = pcd[downsample_idxs, :]\n# batch = torch.zeros(pcd.shape[0]).to(dtype=torch.int64).to(cgn.device)\n# fps_idxs = farthest_point_sample(pcd, batch, 2048 / pcd.shape[0])\n# if obj_mask is not None:\n#     obj_mask = torch.Tensor(obj_mask[downsample_idxs])\n#     obj_mask = obj_mask[fps_idxs]\n# else:\n#     obj_mask = torch.ones(fps_idxs.shape[0])\n# points, pred_grasps, confidence, pred_widths, _, _ = cgn(\n#     pcd[:, 3:],\n#     pcd_poses=pcd[:, :3],\n#     batch=batch,\n#     idxs=fps_idxs,\n#     gripper_depth=gripper_depth,\n#     gripper_width=gripper_width,\n# )\n# sig = torch.nn.Sigmoid()\n# confidence = sig(confidence)\n# confidence = confidence.reshape(-1)\n# pred_grasps = (\n#     torch.flatten(pred_grasps, start_dim=0, end_dim=1).detach().cpu().numpy()\n# )\n# confidence = (\n#     obj_mask.detach().cpu().numpy() * confidence.detach().cpu().numpy()\n# ).reshape(-1)\n# pred_widths = (\n#     torch.flatten(pred_widths, start_dim=0, end_dim=1).detach().cpu().numpy()"
        },
        {
            "comment": "The code segment is responsible for selecting successful grasps based on confidence threshold. It first converts the points to CPU numpy array, then creates a success mask from confidence values above the threshold. If no successful grasps are found, it returns None. Grasps and corresponding confidences are extracted based on the success mask. If maximum number of grasps is specified, it truncates them. Finally, if visualization is enabled, it calls a function to display the grasps on a point cloud. The segment returns the successful grasps, their respective confidence scores, and optionally downsample indexes.",
            "location": "\"/media/root/Prima/works/robo_transformers/docs/src/robo_transformers/inference_server.py\":167-188",
            "content": "# )\n# points = torch.flatten(points, start_dim=0, end_dim=1).detach().cpu().numpy()\n# success_mask = (confidence > threshold).nonzero()[0]\n# if len(success_mask) == 0:\n#     print(\"failed to find successful grasps\")\n#     return None, None, None\n# success_grasps = pred_grasps[success_mask]\n# success_confidence = confidence[success_mask]\n# print(\"Found {} grasps\".format(success_grasps.shape[0]))\n# if max_grasps > 0 and success_grasps.shape[0] > max_grasps:\n#     success_grasps = success_grasps[:max_grasps]\n#     success_confidence = success_confidence[:max_grasps]\n# if visualize:\n#     visualize_grasps(\n#         pcd.detach().cpu().numpy(),\n#         success_grasps,\n#         gripper_depth=gripper_depth,\n#         gripper_width=gripper_width,\n#     )\n# return success_grasps, success_confidence, downsample_idxs"
        }
    ]
}